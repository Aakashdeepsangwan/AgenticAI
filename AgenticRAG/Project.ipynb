{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e500b8e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/akashdeepsangwan/Desktop/Code/GenAI/AgenticAI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import WebBaseLoader, ArxivLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_anthropic import ChatAnthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa3b21e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key= os.getenv(\"claudeAPI\")\n",
        "\n",
        "llm = ChatAnthropic(\n",
        "    model = \"claude-haiku-4-5-20251001\",\n",
        "    temperature = 0.7,\n",
        "    api_key = api_key\n",
        ")\n",
        "\n",
        "Embeddings = HuggingFaceEmbeddings(\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "784e6583",
      "metadata": {},
      "outputs": [],
      "source": [
        "urls=[\n",
        "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
        "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
        "    \"https://docs.langchain.com/oss/python/langgraph/graph-api#map-reduce-and-the-send-api\"\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "chunks = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100).split_documents(docs_list) # each element is a doc\n",
        "vector_store = FAISS.from_documents(chunks,Embeddings)\n",
        "retriever = vector_store.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4c939f8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "​Acknowledgements\n",
            "LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n"
          ]
        }
      ],
      "source": [
        "result =retriever.invoke(\"what is langgraph ?\")\n",
        "print(result[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5b9de95f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retriever_vector_db_blog\",\n",
        "    \"Search and rin information about LangGraph\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f357b85a",
      "metadata": {},
      "source": [
        "### Tool is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1ca8ab4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\nprint(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoChoosing between Graph and Functional APIsPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\\n\\nLangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever_tool.invoke(\"What is langGraph?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fb1ec1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "langchain_url  = [\n",
        "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
        "    \"https://docs.langchain.com/oss/python/langchain/rag\",\n",
        "]\n",
        "\n",
        "web_doc_load = [WebBaseLoader(url).load() for url in langchain_url]\n",
        "langchain_docs = [doc for sublist in web_doc_load for doc in sublist]\n",
        "chunks_lang = RecursiveCharacterTextSplitter(chunk_size =1000, chunk_overlap = 100).split_documents(langchain_docs)\n",
        "vb = FAISS.from_documents(chunks_lang,Embeddings)\n",
        "langchain_retriever = vb.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0e654489",
      "metadata": {},
      "outputs": [],
      "source": [
        "langchain_tool  = create_retriever_tool(\n",
        "    langchain_retriever,\n",
        "    \"retriever_vector_langchain_blog\",\n",
        "    \"Search and run information about LangChain\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1b56e93d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = [retriever_tool, langchain_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d678857",
      "metadata": {},
      "source": [
        "### LangGraph workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d63e476",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1024158d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict) :\n",
        "    messages : Annotated[Sequence[BaseMessage], add_messages]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d68251",
      "metadata": {},
      "source": [
        "## Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3238adfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent(state) :\n",
        "    \"\"\" \n",
        "    Invokes the agent model to generate a response based in the current sate. Given the question, \n",
        "    it will decide the retrieve using the retriever_tool, or simply end.\n",
        "\n",
        "    Args: \n",
        "    state(messages) : The current state\n",
        "\n",
        "    Returns :\n",
        "        dict : The updated state with the agent response attended to messages\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- call agent -----\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    api_key= os.getenv(\"claudeAPI\")\n",
        "\n",
        "    llm = ChatAnthropic(\n",
        "        model = \"claude-haiku-4-5-20251001\",\n",
        "        temperature = 0.7,\n",
        "        api_key = api_key\n",
        "    )\n",
        "\n",
        "\n",
        "    llm= llm.bind_tools(tools)\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\" : [response]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e70ddf1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Annotated, Sequence\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a12e2d",
      "metadata": {},
      "source": [
        "### Grade_Documents Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "be94b161",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts.string import PromptTemplateFormat\n",
        "\n",
        "\n",
        "def grade_documents(state) -> Literal[\"generate\", 'rewrite'] :\n",
        "    \"\"\"  \n",
        "    Determines Whether the retrieved documnets are relevant to the question\n",
        "    \n",
        "    Args :\n",
        "        state(messages) : The Current state\n",
        "\n",
        "    Returns :\n",
        "        str : A decision for whether the documents are relevant or not \n",
        "    \"\"\"\n",
        "    print(\" --- check relevance\")\n",
        "\n",
        "    # Data Model\n",
        "    class grade(BaseModel) :\n",
        "        \"\"\" Binary Score for relevance check \"\"\"\n",
        "        binary_score : str = Field(description = \"Relevance score 'yes' or 'no' \")\n",
        "\n",
        "\n",
        "    api_key= os.getenv(\"claudeAPI\")\n",
        "\n",
        "    llm = ChatAnthropic(\n",
        "        model = \"claude-haiku-4-5-20251001\",\n",
        "        temperature = 0.7,\n",
        "        api_key = api_key\n",
        "    )\n",
        "    \n",
        "\n",
        "    llm_with_tool = llm.with_structured_output(grade)   \n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template = \"\"\" \n",
        "        You are a grader assessing relevance of a retrieved document to a user question.\n",
        "        Here is the retrieved document : \\n\\n {context} \\n\\n\n",
        "        Here is the user question : {question} \\n\n",
        "\n",
        "        If the document contains keywords(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        Give Binary Score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n",
        "        \"\"\",\n",
        "\n",
        "        input_variables= [\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm_with_tool\n",
        "\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # The first question that is being asked will be appended here\n",
        "    question = messages[0].content\n",
        "    docs = last_message.content\n",
        "\n",
        "\n",
        "    scored_result = chain.invoke({\"question\" : question, \"context\" : docs})\n",
        "    score = scored_result.binary_score\n",
        "\n",
        "    if score == \"yes\" :\n",
        "        print(\"-- Decision : DOCS Relevant\")\n",
        "        return \"generate\"    # it will go to the generate node\n",
        "\n",
        "\n",
        "     \n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991c7c44",
      "metadata": {},
      "source": [
        "### Generate Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "189ff7ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "def generate(state) :\n",
        "    \"\"\" \n",
        "    Generate answer\n",
        "\n",
        "    Args :\n",
        "        State(messages) : The current state\n",
        "\n",
        "    Returns :\n",
        "        dict : The updated message\n",
        "    \"\"\"\n",
        "    print(\"--- Generate----\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    docs = last_message.content\n",
        "\n",
        "    # prompt (use LangSmith; langchainhub is deprecated)\n",
        "    \n",
        "    client = Client()\n",
        "    prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
        "\n",
        "    #LLM - already initialised in the other tab\n",
        "    api_key= os.getenv(\"claudeAPI\")\n",
        "\n",
        "    llm = ChatAnthropic(\n",
        "        model = \"claude-haiku-4-5-20251001\",\n",
        "        temperature = 0.7,\n",
        "        api_key = api_key\n",
        "    )\n",
        "    \n",
        "    # Post-processing\n",
        "    \n",
        "    def format_docs(docs) :\n",
        "        return \"\\n\\n\".join(docs.page_content for doc in docs)\n",
        "    \n",
        "    #chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "    # Run\n",
        "    response = rag_chain.invoke({\"context\" : docs, \"question\" : question})\n",
        "    return {\"messages\" : [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcbc47c7",
      "metadata": {},
      "source": [
        "### ReWrite Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "466747f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rewrite(state) :\n",
        "    \n",
        "    \"\"\" \n",
        "    Transform the query to produce the better answer\n",
        "\n",
        "    Args : \n",
        "        state (messages) : The current state\n",
        "    \n",
        "    Returns :\n",
        "        dict : The updated state with re-phrased question    \n",
        "    \"\"\"\n",
        "\n",
        "    print(\"----Transform Query----\")\n",
        "    messages = state['messages']\n",
        "    question = messages[0].content\n",
        "\n",
        "\n",
        "    api_key= os.getenv(\"claudeAPI\")\n",
        "\n",
        "    llm = ChatAnthropic(\n",
        "        model = \"claude-haiku-4-5-20251001\",\n",
        "        temperature = 0.7,\n",
        "        api_key = api_key\n",
        "    )\n",
        "\n",
        "    msg = [\n",
        "        HumanMessage(\n",
        "            content = \"\"\"\n",
        "            Look at the input and try to reason about the underlying semantic intent/meaning.\n",
        "            Here is the initial question :\n",
        "            \\n ------- \\n\n",
        "            {question}\n",
        "            \\n --------- \\n\n",
        "            Formulate an improved question : \n",
        "            \"\"\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(msg)\n",
        "    return {\"messages\" : [response]}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e194b8c8",
      "metadata": {},
      "source": [
        "### Graph WorkFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b402aed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "\n",
        "#Nodes\n",
        "workflow.add_node(\"agent\", agent)\n",
        "retrieve = ToolNode(tools)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"rewrite\", rewrite) # Re-writing the question\n",
        "workflow.add_node(\"generate\", generate)\n",
        "\n",
        "\n",
        "# Edges\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\", tools_condition, {\"tools\" : \"retrieve\", END : END},\n",
        ")\n",
        "\n",
        "\n",
        "# Edge taken after the 'action' node is called. \n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\", grade_documents\n",
        ")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n",
        "\n",
        "# compile\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a5b5512b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- call agent -----\n",
            "-- Decision : DOCS Relevant\n",
            "--- Generate----\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is langgraph ?', additional_kwargs={}, response_metadata={}, id='52dc339c-90f2-48bb-a008-705b9da41f2a'),\n",
              "  AIMessage(content=[{'id': 'toolu_015bX4eP1fRNBUw3fvZXw52v', 'input': {'query': 'what is langgraph'}, 'name': 'retriever_vector_db_blog', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01SBPngeTvV6WsWdzYDki9m4', 'model': 'claude-haiku-4-5-20251001', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 658, 'output_tokens': 63, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-haiku-4-5-20251001', 'model_provider': 'anthropic'}, id='lc_run--019c078f-86a9-7751-ac37-d03596f7733d-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'what is langgraph'}, 'id': 'toolu_015bX4eP1fRNBUw3fvZXw52v', 'type': 'tool_call'}], usage_metadata={'input_tokens': 658, 'output_tokens': 63, 'total_tokens': 721, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}),\n",
              "  ToolMessage(content='\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nprint(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoChoosing between Graph and Functional APIsPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\\n\\nLangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful\\n\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph', name='retriever_vector_db_blog', id='8b33923a-ac1e-4244-b1db-8e8b369fc70c', tool_call_id='toolu_015bX4eP1fRNBUw3fvZXw52v'),\n",
              "  HumanMessage(content='LangGraph is a low-level orchestration framework and runtime built by LangChain Inc for building, managing, and deploying long-running, stateful agents. It focuses on agent orchestration capabilities such as durable execution, streaming, human-in-the-loop interactions, and can be used independently without LangChain. LangGraph is inspired by Pregel and Apache Beam, and comes with built-in visualization tools for graph management.', additional_kwargs={}, response_metadata={}, id='7bde9b93-8e15-4286-9b27-c1ae682ee330')]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.invoke({\"messages\": \"what is langgraph ?\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b58fde58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- call agent -----\n",
            "-- Decision : DOCS Relevant\n",
            "--- Generate----\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is langchain?', additional_kwargs={}, response_metadata={}, id='7ed5002d-68d4-49ed-926b-7de1b07d8c22'),\n",
              "  AIMessage(content=[{'id': 'toolu_01MNFjQshrSWQHvUW6uJZRj6', 'input': {'query': 'what is langchain'}, 'name': 'retriever_vector_langchain_blog', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_011dTxNpPfo2W85ZZfT474tT', 'model': 'claude-haiku-4-5-20251001', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 657, 'output_tokens': 63, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-haiku-4-5-20251001', 'model_provider': 'anthropic'}, id='lc_run--019c0790-6178-7652-9b91-ce92d153ea39-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'what is langchain'}, 'id': 'toolu_01MNFjQshrSWQHvUW6uJZRj6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 657, 'output_tokens': 63, 'total_tokens': 720, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}),\n",
              "  ToolMessage(content='from langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nBuild a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsLangChain vs. LangGraph vs. Deep AgentsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\n\\nLangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easiest way to start building agents and', name='retriever_vector_langchain_blog', id='c3dc17db-8269-4208-afcd-827a213721b2', tool_call_id='toolu_01MNFjQshrSWQHvUW6uJZRj6'),\n",
              "  HumanMessage(content='LangChain is an open source framework that provides a pre-built agent architecture and integrations for any model or tool, making it easy to build agents that can adapt quickly to ecosystem changes. It offers core components like agents, models, messages, and tools to help developers create complex applications with multiple LLM invocations. LangChain also integrates with LangSmith for observability and tracing to inspect and debug chain and agent behavior.', additional_kwargs={}, response_metadata={}, id='1bbdb283-51c3-48a7-a3ee-d7c03f40ee93')]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.invoke({\"messages\" : \"what is langchain?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83de620d",
      "metadata": {},
      "source": [
        "#### Not calling any tool here, it is directly using the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a17f8db8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- call agent -----\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is Machine Learning ?', additional_kwargs={}, response_metadata={}, id='11812ca8-41b4-4e57-832b-5320675284d9'),\n",
              "  AIMessage(content=\"Machine Learning is a branch of artificial intelligence (AI) that focuses on enabling computers to learn and improve from experience without being explicitly programmed for every scenario.\\n\\nHere are the key aspects of Machine Learning:\\n\\n## Core Concept\\nMachine Learning systems learn patterns from data and use those patterns to make predictions or decisions on new, unseen data. Instead of following pre-written rules, ML algorithms identify rules and patterns automatically.\\n\\n## Main Types of Machine Learning\\n\\n1. **Supervised Learning** - Learning from labeled data where the correct answers are provided\\n   - Examples: Classification, Regression\\n   - Used for: Predicting house prices, spam detection, image recognition\\n\\n2. **Unsupervised Learning** - Learning from unlabeled data to find hidden patterns\\n   - Examples: Clustering, Dimensionality Reduction\\n   - Used for: Customer segmentation, anomaly detection\\n\\n3. **Reinforcement Learning** - Learning through interaction and rewards/penalties\\n   - Examples: Game playing, Robotics\\n   - Used for: Autonomous systems, game AI\\n\\n## Key Applications\\n- Natural Language Processing (NLP)\\n- Computer Vision\\n- Recommendation Systems\\n- Predictive Analytics\\n- Autonomous Vehicles\\n- Healthcare Diagnostics\\n\\n## Why It Matters\\nMachine Learning enables systems to:\\n- Adapt to new data automatically\\n- Handle complex patterns humans might miss\\n- Scale solutions without manually coding every rule\\n- Improve performance over time with more data\\n\\nIs there a specific aspect of Machine Learning you'd like to learn more about, such as how it relates to LangChain or LangGraph for building AI applications?\", additional_kwargs={}, response_metadata={'id': 'msg_016x9ts93ebp69JzVgmMy9Vh', 'model': 'claude-haiku-4-5-20251001', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 657, 'output_tokens': 354, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-haiku-4-5-20251001', 'model_provider': 'anthropic'}, id='lc_run--019c0790-edf9-7961-aa6b-5c002481ebc5-0', usage_metadata={'input_tokens': 657, 'output_tokens': 354, 'total_tokens': 1011, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.invoke({\"messages\": \"what is Machine Learning ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426b3167",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
